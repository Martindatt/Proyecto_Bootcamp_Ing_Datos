# ğŸš€ Proyecto ETL - In-nova Tech (DummyJSON API)

Este proyecto realiza la extracciÃ³n, limpieza y guardado de datos provenientes de la API pÃºblica de DummyJSON, simulando un pipeline real de e-commerce para la empresa ficticia **In-nova Tech**.

### ğŸ§© APIs utilizadas:

- `/products` â†’ catÃ¡logo de productos
- `/carts` â†’ carritos de compras (nivel Ã­tem)
- `/users` â†’ clientes

---

## âœ… Â¿QuÃ© se limpiÃ³ y por quÃ©?

### ğŸ›ï¸ Productos (`/products`)

**Objetivo:** garantizar que todos los productos sean vÃ¡lidos para anÃ¡lisis de ventas.

**Limpiezas aplicadas:**

- Se seleccionaron las columnas relevantes (`id`, `title`, `price`, `discountPercentage`, etc.).
- Se convirtieron a tipos numÃ©ricos (`precio`, `stock`, `rating`, `descuento_pct`) para evitar errores de cÃ¡lculo.
- Se eliminaron productos con:
  - `precio` nulo o menor a 0.
- Se imputÃ³ `descuento_pct` faltante con `0`.
- Se completÃ³ `categoria` con `"Sin categorÃ­a"` cuando venÃ­a vacÃ­a.
- Se eliminaron duplicados por `product_id`.

---

### ğŸ›’ Carritos (`/carts`)

**Objetivo:** obtener una tabla detallada por producto en el carrito, con cantidades y totales.

**Limpiezas aplicadas:**

- Se aplanaron los carritos para obtener una fila por producto (`cart_id`, `product_id`, `cantidad`, etc.).
- Se eliminaron lÃ­neas con `cantidad <= 0`.
- Se imputÃ³ `descuento_pct` faltante con `0`.
- Se redondearon los campos `total` y `total_con_descuento` a 2 decimales.
- Se eliminaron duplicados por combinaciÃ³n (`cart_id`, `product_id`) para evitar lÃ­neas repetidas exactas en un mismo carrito.

---

### ğŸ‘¤ Usuarios (`/users`)

**Objetivo:** conservar usuarios vÃ¡lidos para anÃ¡lisis de comportamiento y vinculaciÃ³n con carritos.

**Limpiezas aplicadas:**

- Se seleccionaron campos clave (`id`, `nombre`, `apellido`, `email`, `ciudad`, etc.).
- Se normalizÃ³ el `email` (`lower`, sin espacios).
- Se agregÃ³ una columna `email_valido`:
  - `True` si el email existe y no estÃ¡ vacÃ­o.
  - `False` si falta o estÃ¡ en blanco.
- Se conservaron todos los usuarios (incluso los sin email), para trazabilidad.
- Se eliminaron duplicados por `user_id` y por `email`.

---

## ğŸ“ Archivos generados (outputs/)

- `products_clean.csv`
- `carts_clean.csv`
- `users_clean.csv`

---

## ğŸ§® Transformaciones SQL con DuckDB

En esta fase se consolidÃ³ todo el pipeline de datos usando **DuckDB** como motor local para ejecutar consultas.
El objetivo fue unir las tablas principales (usuarios, carritos y productos), calcular mÃ©tricas reales de negocio e iniciar la preparaciÃ³n del modelo analÃ­tico.

Se creÃ³ un script Ãºnico `transformaciones.sql` que:

1. Lee los CSV limpios (productos, usuarios y carritos)
2. Construye la tabla de hechos `fact_ventas`

   - Unen los hechos de compras -Carts- con Productos y Clientes.
   - Se obtienen metricas relevantes para un analisis de ventas.

3. Genera el agregado `ventas_por_categoria`

   - Se agrupan las ventas por categoria de productos.
   - Se incluyen metricas para un analisis a nivel venta/categoria de producto.

4. Exporta ambas tablas a la carpeta `outputs/`

## ğŸ“ Archivos generados (outputs/)

- `fact_ventas.csv`
- `ventas_por_categoria.csv`

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Python** â†’ extracciÃ³n y limpieza con `requests` y `pandas`
- **DuckDB (SQL)** â†’ modelado de hechos y agregaciones
- **Pandas** â†’ normalizaciÃ³n y validaciones
- **CSV / ETL modular** â†’ outputs ordenados por fase

## ğŸ§  QuÃ© aprendÃ­

- A utilizar **DuckDB** como motor analÃ­tico local sobre archivos CSV.
- A modelar una **tabla de hechos** e integrar dimensiones (productos y usuarios).
- A calcular mÃ©tricas reales de negocio: ingresos, descuentos y unidades vendidas.
- A estructurar un **flujo ETL SQL modular**, reproducible y automatizado desde Python.
- A documentar y versionar un pipeline siguiendo **buenas prÃ¡cticas de ingenierÃ­a de datos**.

## âš™ï¸ Ejecutar todo el pipeline SQL

```bash
python3 scripts_sql/run_sql.py
```
