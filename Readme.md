# ğŸš€ Proyecto ETL - In-nova Tech (DummyJSON API)

Este proyecto realiza la extracciÃ³n, limpieza y guardado de datos provenientes de la API pÃºblica de DummyJSON, simulando un pipeline real de e-commerce para la empresa ficticia **In-nova Tech**.

### ğŸ§© APIs utilizadas:

- `/products` â†’ catÃ¡logo de productos
- `/carts` â†’ carritos de compras (nivel Ã­tem)
- `/users` â†’ clientes

---

## âœ… Â¿QuÃ© se limpiÃ³ y por quÃ©?

### ğŸ›ï¸ Productos (`/products`)

**Objetivo:** garantizar que todos los productos sean vÃ¡lidos para anÃ¡lisis de ventas.

**Limpiezas aplicadas:**

- Se seleccionaron las columnas relevantes (`id`, `title`, `price`, `discountPercentage`, etc.).
- Se convirtieron a tipos numÃ©ricos (`precio`, `stock`, `rating`, `descuento_pct`) para evitar errores de cÃ¡lculo.
- Se eliminaron productos con:
  - `precio` nulo o menor a 0.
- Se imputÃ³ `descuento_pct` faltante con `0`.
- Se completÃ³ `categoria` con `"Sin categorÃ­a"` cuando venÃ­a vacÃ­a.
- Se eliminaron duplicados por `product_id`.

---

### ğŸ›’ Carritos (`/carts`)

**Objetivo:** obtener una tabla detallada por producto en el carrito, con cantidades y totales.

**Limpiezas aplicadas:**

- Se aplanaron los carritos para obtener una fila por producto (`cart_id`, `product_id`, `cantidad`, etc.).
- Se eliminaron lÃ­neas con `cantidad <= 0`.
- Se imputÃ³ `descuento_pct` faltante con `0`.
- Se redondearon los campos `total` y `total_con_descuento` a 2 decimales.
- Se eliminaron duplicados por combinaciÃ³n (`cart_id`, `product_id`) para evitar lÃ­neas repetidas exactas en un mismo carrito.

---

### ğŸ‘¤ Usuarios (`/users`)

**Objetivo:** conservar usuarios vÃ¡lidos para anÃ¡lisis de comportamiento y vinculaciÃ³n con carritos.

**Limpiezas aplicadas:**

- Se seleccionaron campos clave (`id`, `nombre`, `apellido`, `email`, `ciudad`, etc.).
- Se normalizÃ³ el `email` (`lower`, sin espacios).
- Se agregÃ³ una columna `email_valido`:
  - `True` si el email existe y no estÃ¡ vacÃ­o.
  - `False` si falta o estÃ¡ en blanco.
- Se conservaron todos los usuarios (incluso los sin email), para trazabilidad.
- Se eliminaron duplicados por `user_id` y por `email`.

---

## ğŸ“ Archivos generados (outputs/silver)

- `products_clean.csv`
- `carts_clean.csv`
- `users_clean.csv`

---

## ğŸ§® Transformaciones SQL con DuckDB

En esta fase se consolidÃ³ todo el pipeline de datos usando **DuckDB** como motor local para ejecutar consultas.
El objetivo fue unir las tablas principales (usuarios, carritos y productos), calcular mÃ©tricas reales de negocio e iniciar la preparaciÃ³n del modelo analÃ­tico.

Se creÃ³ un script Ãºnico `transformaciones.sql` que:

1. Carga de fuentes limpias (silver/) como vistas (v_products, v_carts, v_users).

2. ValidaciÃ³n de integridad referencial (FKs):

- Detecta carritos con product_id o user_id inexistentes.
- Exporta auditorÃ­as (fk_prod_missing.csv, fk_user_missing.csv).

3. CreaciÃ³n de la tabla de hechos `fact_ventas`:

- Solo incluye filas con FKs vÃ¡lidas.
- Calcula mÃ©tricas de negocio: ingreso_bruto, ingreso_neto, descuento_importe.

4. GeneraciÃ³n de la tabla analÃ­tica `ventas_por_categoria`:

- Agrega mÃ©tricas por categorÃ­a.
- Incluye anÃ¡lisis adicional con CTEs:
  - % de participaciÃ³n por categorÃ­a.
  - Cantidad y porcentaje de productos sobre el promedio de su categorÃ­a.

5. ğŸ“ Export de resultados (gold/):

- `fact_ventas.csv`
- `ventas_por_categoria.csv`

6. ğŸ“ Export de auditorÃ­as (auditoria/):

- Reportes de FKs faltantes y conteos de control.

--- 

## ğŸ§± Modelo Estrella - Datawharehouse

En esta fase se implementÃ³ el modelo de datos analÃ­tico bajo un esquema estrella, que permite analizar mÃ©tricas de ventas desde distintas perspectivas: producto, cliente y tiempo.

El script `create_schema.sql` crea la capa DW (Data Warehouse) dentro del archivo innova_tech_dw.duckdb, a partir de las tablas generadas en la etapa anterior.

## ğŸ“Š Tablas creadas

1ï¸âƒ£ Dimensiones

- **dim_usuario**
Contiene informaciÃ³n descriptiva de cada cliente:
user_id, nombre, apellido, email, ciudad, provincia, cp, direccion.

- **dim_producto**
Describe los productos y su clasificaciÃ³n:
product_id, nombre_producto, categoria, marca.

- **dim_fecha**
Estructura de calendario para anÃ¡lisis temporal:
fecha_id, anio, mes, nombre_mes, trimestre, dia, dia_semana.
Los registros se generan automÃ¡ticamente a partir de las fechas presentes en fact_ventas_dw.

2ï¸âƒ£ Tabla de Hechos

**fact_ventas_dw**
Registra cada lÃ­nea de venta (nivel Ã­tem de carrito) y enlaza con las dimensiones.
MÃ©tricas calculadas:

cantidad

precio_unitario

ingreso_bruto

descuento_importe

ingreso_neto

descuento_pct

---

## ğŸ”— Relaciones

       dim_usuario          dim_producto
       (user_id PK)         (product_id PK)
             â”‚                    â”‚
             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
              fact_ventas_dw
                     â”‚
                     â–¼
                dim_fecha


---

## ğŸ“ˆ MÃ©tricas posibles

- Ventas totales por mes / trimestre / aÃ±o

- Ingresos brutos / netos por categorÃ­a o marca

- Top usuarios y productos mÃ¡s vendidos

- Ticket promedio (ingreso_neto / cantidad)

- % de descuento aplicado por periodo o categorÃ­a

- Comparativos YoY / QoQ (aÃ±o contra aÃ±o, trimestre contra trimestre)


## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Python** â†’ extracciÃ³n y limpieza de APIs con `requests` y `pandas`.
- **DuckDB (SQL)** â†’ modelado analÃ­tico y transformaciones.
- **Pandas** â†’ normalizaciÃ³n y validaciones.
- **Logging + AudtiorÃ­a FK** â†’ validaciÃ³n de integridad previa.
- **CSV / ETL modular** â†’ outputs ordenados por fase y separaciÃ³n clara de capas de datos.

## ğŸ§  QuÃ© aprendÃ­

- A estructurar un **pipeline ETL** completo y modular, desde la extracciÃ³n hasta la capa analÃ­tica.
- A utilizar **DuckDB** como motor SQL embebido para procesamiento local de datos.
- A aplicar **validaciones FK** antes de generar tablas de hechos.
- A usar **CTEs y funciones VIEW** para cÃ¡lculos analÃ­ticos (% participaciÃ³n, productos sobre promedio).
- A organizar la informaciÃ³n bajo una **arquitectura Silver / Gold / AuditorÃ­a**.
- A documentar y versionar un pipeline siguiendo **buenas prÃ¡cticas de ingenierÃ­a de datos**.

## âš™ï¸ Ejecutar todo el pipeline SQL

```bash
python3 scripts_sql/run_sql.py
```
